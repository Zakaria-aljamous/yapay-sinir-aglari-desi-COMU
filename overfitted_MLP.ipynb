{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaPcZcuWTd3U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(170401085)\n",
        "\n",
        "# create empty lists to store the training, validation, and test losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "# Upload the train and validation datasets\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the train and validation datasets into Pandas DataFrames\n",
        "train_df = pd.read_csv('cure_the_princess_train.csv')\n",
        "val_df = pd.read_csv('cure_the_princess_validation.csv')\n",
        "test_df=pd.read_csv('cure_the_princess_test.csv')\n",
        "\n",
        "# Extract the features and labels from the train and validation DataFrames\n",
        "train_data = train_df.drop(['Cured'], axis=1).to_numpy()\n",
        "train_labels = train_df['Cured'].to_numpy().reshape(-1, 1).astype(float)\n",
        "val_data = val_df.drop(['Cured'], axis=1).to_numpy()\n",
        "val_labels = val_df['Cured'].to_numpy().reshape(-1, 1).astype(float)\n",
        "test_data = test_df.drop(['Cured'], axis=1).to_numpy()\n",
        "test_labels = test_df['Cured'].to_numpy().reshape(-1, 1).astype(float)\n",
        "\n",
        "# Convert the NumPy arrays to PyTorch Tensors\n",
        "train_data = torch.from_numpy(train_data).float()\n",
        "train_labels = torch.from_numpy(train_labels).float()\n",
        "val_data = torch.from_numpy(val_data).float()\n",
        "val_labels = torch.from_numpy(val_labels).float()\n",
        "test_data = torch.from_numpy(test_data).float()\n",
        "test_labels = torch.from_numpy(test_labels).float()\n",
        "\n",
        "# Create the training and validation datasets and test data\n",
        "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
        "val_dataset = torch.utils.data.TensorDataset(val_data, val_labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
        "\n",
        "# Create the training and validation dataloaders and test data \n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(13, 200)\n",
        "        self.fc2 = nn.Linear(200, 100)\n",
        "        self.fc3 = nn.Linear(100, 50)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(),  lr=0.01, weight_decay=0.001)\n",
        "\n",
        "# Train and validation loops\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "   \n",
        " \n",
        "    for inputs, labels in train_loader:\n",
        " \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.unsqueeze(1)  # add this line to flatten the target tensor\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    \n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "        \n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss / len(train_loader)\n",
        "\n",
        "def val_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "\n",
        "# loop through the dataset for each epoch and compute the training and validation losses\n",
        "for epoch in range(50):\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss = val_epoch(model, val_loader, criterion)\n",
        "\n",
        "    # add the training and validation losses to their respective lists\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # compute the testing loss and add it to the test loss list\n",
        "    test_loss = val_epoch(model, test_loader, criterion)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    # print the training and validation loss for each epoch\n",
        "    print(f'Epoch {epoch + 1}: Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}, Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# plot the training, validation, and test losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.plot(test_losses, label='Testing Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Define the model and load the best state dict obtained through early stopping\n",
        "model = MLP()\n",
        "# best_model_path = 'best_model.pth'\n",
        "# model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "# Define a function to calculate evaluation metrics\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        true_positives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "        true_negatives = 0\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "            false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "            false_negatives += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "            true_negatives += ((predicted == 0) & (labels == 0)).sum().item()\n",
        "    \n",
        "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + false_negatives + true_negatives)\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    \n",
        "    return accuracy, f1, precision, recall\n",
        "\n",
        "# Calculate evaluation metrics on the test set\n",
        "test_accuracy, test_f1, test_precision, test_recall = evaluate(model, test_loader)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "print(f'Test F1 Score: {test_f1:.4f}')\n",
        "print(f'Test Precision: {test_precision:.4f}')\n",
        "print(f'Test Recall: {test_recall:.4f}')\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# check if GPU is available\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "# move model and data to GPU device\n",
        "model.to(device)\n",
        "train_loader.dataset.tensors = tuple(t.to(device) for t in train_loader.dataset.tensors)\n",
        "val_loader.dataset.tensors = tuple(t.to(device) for t in val_loader.dataset.tensors)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss = val_epoch(model, val_loader, criterion)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Execution time CPU: {end_time - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "GM84uTYXLA79"
      }
    }
  ]
}